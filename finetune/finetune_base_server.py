# -*- coding: utf-8 -*-
"""finetune_llama2_base data 테스트.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUAdJus9HbyeeeHv1eFuuCrB-FtMKc7F
"""

#!pip install accelerate peft bitsandbytes transformers trl

#!pip install --upgrade datasets

# Model from Hugging Face hub
base_model = "NousResearch/Llama-2-7b-chat-hf"

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer

#!pip install bitsandbytes==0.39.0

'''
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)'''

#!pip install accelerate==0.21.0

#!pip install -U accelerate

#!pip install -i https://pypi.org/simple/ bitsandbytes







data = load_dataset("csv", data_files="./data/base0518_inout_train.csv")
val_data =load_dataset("csv", data_files="./data/base0518_inout_val.csv")


data['train'][0]

data['train'][1]

device = "cuda" if torch.cuda.is_available() else "cpu" #will not use gpu right now
device ="cpu"


model = AutoModelForCausalLM.from_pretrained(
    base_model,
    #quantization_config=quant_config,
    #device_map = "auto"
    #device_map={"": 0}
    device_map="cpu"
)
#device = torch.device('cpu')
#model = model.to(device)
model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

def tokenize_function(examples):
    inputs = tokenizer(examples["input"], padding="max_length", truncation=True, max_length=512)
    outputs = tokenizer(examples["output"], padding="max_length", truncation=True, max_length=512)
    inputs['labels'] = outputs['input_ids']
    return inputs



tokenized_train_dataset = data['train'].map(tokenize_function, batched=True)
tokenized_val_dataset = val_data['train'].map(tokenize_function, batched=True)


peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=7,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard",
    no_cuda=True #train in CPU
)

training_params = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=7,
    weight_decay=0.01,
    #remove_unused_columns=False
)

from transformers import Trainer

'''
trainer = SFTTrainer(
    model=model,
    train_dataset=data['train'],
    eval_dataset=val_data['train'],
    #peft_config=peft_params,
    dataset_text_field="preprocessed_input",
    #max_seq_length=512,
    tokenizer=tokenizer,
    args=training_params,
    #packing=False,
)'''
trainer = Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    #peft_config=peft_params,
    #dataset_text_field="preprocessed_input",
    #max_seq_length=512,
    tokenizer=tokenizer,
    args=training_params,
    #packing=False,
)

print(data['train'][:5])
#print(val_data['train'][:5])

trainer.train()

new_model = "./results/llama-base"

trainer.model.save_pretrained(new_model)
trainer.tokenizer.save_pretrained(new_model)

logging.set_verbosity(logging.CRITICAL)

prompt1 = "[INST]Given a raw source for a product, generate a coherent and relevant product title in Korean.[/INST]새로운 스마트워치 2023 터치 스크린 운동 모니터링 심박수 혈압 수면 추적 다기능 남녀 공용 블루투스 팔찌"

prompt2 = "[INST]Given a raw source for a product, generate five detailed descriptions in Korean. Each description should be korean and relevant to the raw source.[/INST]창의적인 다기능 책상 정리함 사무실 학생 문구함 가정용 미니 서랍형 화장품 정리함"

#prompt = "[INST]Given a raw source for a product, generate five detailed descriptions in Korean.[/INST]창의적인 다기능 책상 정리함 사무실 학생 문구함 가정용 미니 서랍형 화장품 정리함"

input_text = f"{prompt1}"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

with torch.no_grad():
    output_ids = model.generate(
        input_ids,
        max_length=512,
        do_sample=True,  # Enable sampling if you want diverse results
        temperature=0.9,  # Adjust based on desired creativity
        top_p=0.95,
        top_k=50
    )
response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(response)

input_text = f"{prompt2}"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

with torch.no_grad():
    output_ids = model.generate(
        input_ids,
        max_length=512,
        do_sample=True,  # Enable sampling if you want diverse results
        temperature=0.9,  # Adjust based on desired creativity
        top_p=0.95,
        top_k=50
    )
response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(response)




